{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "\n",
    "\n",
    "During this classes we implement few ensemble methods:\n",
    "* boosting,\n",
    "* bagging.\n",
    "\n",
    "There are more explained in the lectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "AdaBoost consists of following steps:\n",
    "* initialize weights to $\\frac{1}{N}$, where $N$ is the number of datapoints,\n",
    "* loop until \n",
    "  $\\varepsilon_{t}<\\frac{1}{2}$\n",
    "  or maximum number of iteration is reached,\n",
    "\n",
    "* train classifier on ${S,w^{(t)}}$ and get a hypothesis $h_{t}(x_{n})$ for datapoints $x_{n}$,\n",
    "\n",
    "* compute error $\\varepsilon_{t}=\\sum_{n=1}^{N}w_{n}^{(t)}I(y_{n}\\neq h_{t}(x_{n}))$,       \n",
    "\n",
    "* set $\\alpha_{t}=\\log(\\frac{1-\\varepsilon_{t}}{\\varepsilon_{t}})$.\n",
    "  \n",
    "* update weights $w_{n}^{(t+1)}=\\frac{w_{n}^{(t)}\\exp{\\alpha_{t}I(y_{n}\\neq h_{t}(x_{n}))}}{Z_{t}}$,\n",
    "  where $Z_{t}$ is a normalization constant,\n",
    "\n",
    "* output $f(X)=\\text{sign}(\\sum_{t=1}^{T}\\alpha_{t}h_{t}(x))$.\n",
    "  \n",
    "Example taken from Marsland, Machine Learning: https://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html.\n",
    "\n",
    "\n",
    "First, we need to import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The training part consis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data,classes,weights,whichdim):\n",
    "    error = np.zeros(10)\n",
    "    for value in range(0,10,1):\n",
    "        val = float(value)/10\n",
    "        classn = np.where(data[whichdim,:]<val,-1,1)\n",
    "        ind = np.where(classes!=classn)\n",
    "        error[value] = np.sum(weights[ind])\n",
    "\n",
    "    return whichdim,float(np.argmin(error))/10,1-whichdim  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data,classes,dim,value):\n",
    "    classn = np.where(data[int(dim),:]<value,-1,1)\n",
    "    ind = np.where(classes!=classn,1,0)\n",
    "    return classn, ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boost(data,classes,testdata,testclasses):\n",
    "    T = 20\n",
    "    N = np.shape(data)[1]\n",
    "    ndim = np.shape(data)[0]\n",
    "    classifiers = np.zeros((2,T))\n",
    "    whichdim = 0\n",
    "\n",
    "    w = np.ones((N,T+1),dtype=float)/N\n",
    "    index = np.ones((N,T+1))\n",
    "    e = np.zeros(T)\n",
    "    alpha = np.zeros(T+1)\n",
    "\n",
    "    err = np.zeros((2,T+1))\n",
    "\n",
    "    poutput = np.zeros((T+1,N))\n",
    "    ptoutput = np.zeros((T+1,N))\n",
    "    po = np.zeros(T+1)\n",
    "    pto = np.zeros(T+1)\n",
    "\n",
    "    for t in range(T):\n",
    "        classifiers[0,t],classifiers[1,t],whichdim = train(data,classes,w[:,t],whichdim)\n",
    "        #print \"Out\", classifiers[:,t]\n",
    "        outputs,errors = classify(data,classes,classifiers[0,t],classifiers[1,t])\n",
    "        toutputs,terrors = classify(testdata,testclasses,classifiers[0,t],classifiers[1,t])\n",
    "\n",
    "        which = np.where(outputs<=0)\n",
    "        which2 = np.where(outputs>0)\n",
    "        #pl.figure()\n",
    "        #pl.plot(data[0,which],data[1,which],'ko',ms=15)\n",
    "        #pl.plot(data[0,which2],data[1,which2],'k^',ms=15)\n",
    "        index[:,t] = errors\n",
    "        #print \"index: \", index[:,t]\n",
    "        #print \"e: \", w[:,t] * index[:,t]\n",
    "        e[t] = np.sum(w[:,t]*index[:,t])/np.sum(w[:,t])\n",
    "        #print \"e: \",e[t]\n",
    "            \n",
    "        if t>0 and (e[t]==0 or e[t]>=0.5):\n",
    "            T=t\n",
    "            alpha = alpha[:t]\n",
    "            index = index[:,:t]\n",
    "            w = w[:,:t]\n",
    "            break\n",
    "\n",
    "        alpha[t] = np.log((1-e[t])/e[t])\n",
    "        #print \"alpha: \", alpha[t]\n",
    "        w[:,t+1] = w[:,t]* np.exp(alpha[t]*index[:,t])\n",
    "        w[:,t+1] = w[:,t+1]/np.sum(w[:,t+1])\n",
    "        #print \"w: \", w[:,t+1], sum(w[:,t+1])\n",
    "              \n",
    "        outputs = np.zeros((N,t))\n",
    "        toutputs = np.zeros((N,t))\n",
    "        for i in range(t):\n",
    "            outputs[:,i],errors  = classify(data,classes,classifiers[0,i],classifiers[1,i])\n",
    "            toutputs[:,i],terrors  = classify(testdata,testclasses,classifiers[0,i],classifiers[1,i])\n",
    "    \n",
    "        for n in range(N):\n",
    "            poutput[t,n] = np.sum(alpha[:t]*outputs[n,:])/sum(alpha)\n",
    "            ptoutput[t,n] = np.sum(alpha[:t]*toutputs[n,:])/sum(alpha)\n",
    "        poutput[t,:] = np.where(poutput[t,:]>0,1,-1)\n",
    "        ptoutput[t,:] = np.where(ptoutput[t,:]>0,1,-1)\n",
    "        po[t] = np.shape(np.where(poutput[t,:]!=classes))[1]\n",
    "        pto[t] = np.shape(np.where(ptoutput[t,:]!=testclasses))[1]\n",
    "    #print \"output: \"\n",
    "    #print alpha\n",
    "    outputs = np.zeros((N,np.shape(w)[1]))\n",
    "    for t in range(T):\n",
    "        outputs[:,t],errors  = classify(data,classes,classifiers[0,t],classifiers[1,t])\n",
    "    \n",
    "    output = np.zeros(N)\n",
    "    for n in range(N):\n",
    "        output[n] = np.sum(alpha*outputs[n,:])/np.sum(alpha)\n",
    "\n",
    "        \n",
    "    #print output\n",
    "    #print classes \n",
    "    which = np.where(output<=0)\n",
    "    which2 = np.where(output>0)\n",
    "    pl.figure()\n",
    "    pl.plot(data[0,which],data[1,which],'ko',ms=15)\n",
    "    pl.plot(data[0,which2],data[1,which2],'k^',ms=15)\n",
    "    pl.title('Output on training data')\n",
    "    #axis('off')        \n",
    "        \n",
    "    outputs = np.zeros((N,np.shape(w)[1]))\n",
    "    for t in range(T):\n",
    "        outputs[:,t],errors  = classify(testdata,testclasses,classifiers[0,t],classifiers[1,t])\n",
    "    \n",
    "    output = np.zeros(N)\n",
    "    for n in range(N):\n",
    "        output[n] = np.sum(alpha*outputs[n,:])/np.sum(alpha)          \n",
    "    which = np.where(output<=0)\n",
    "    which2 = np.where(output>0)\n",
    "    pl.figure()\n",
    "    pl.title('Output on test data')\n",
    "    pl.plot(testdata[0,which],testdata[1,which],'ko',ms=15)\n",
    "    pl.plot(testdata[0,which2],testdata[1,which2],'k^',ms=15)\n",
    "        \n",
    "    pl.figure()\n",
    "    pl.plot(np.arange(T),po[:T]/N,'k-',np.arange(T),pto[:T]/N,'k--')\n",
    "    pl.legend(('Training error','Test error'))\n",
    "    pl.xlabel('Iterations')\n",
    "    pl.ylabel('Error')    \n",
    "    return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_boost():\n",
    "    pl.ion()\n",
    "    ndata = 50\n",
    "    data = np.random.rand(2,ndata)\n",
    "    #which = where(data[0,:]>0.4)\n",
    "    #which2 = where(data[0,:]<=0.4)\n",
    "    classes = np.where(((data[0,:]>0.4) & (data[1,:]>0.4)),1,-1)\n",
    "\n",
    "\n",
    "    #classes = where(((data[0,:]>0.7) & (data[1,:]>0.7)) | ((data[0,:]<0.3) & (data[1,:]<0.3)),1,-1)\n",
    "    \n",
    "    #false = where(data[0,:]<0.3)\n",
    "    #new = random.randint(len(false))\n",
    "    #classes[false[0][new]] = 1\n",
    "    \n",
    "    which = np.where(classes==-1)\n",
    "    which2 = np.where(classes==1)\n",
    "    pl.plot(data[0,which],data[1,which],'ko',ms=15)\n",
    "    pl.plot(data[0,which2],data[1,which2],'k^',ms=15)\n",
    "    pl.title('Training Data')\n",
    "    testdata = np.random.rand(2,ndata)\n",
    "    testclasses = np.where(((testdata[0,:]>0.4) & (testdata[1,:]>0.4)),1,-1)\n",
    "    boost(data,classes,testdata,testclasses)\n",
    "    \n",
    "    pl.figure()\n",
    "    pl.title('Test Data')\n",
    "    which = np.where(testclasses==-1)\n",
    "    which2 = np.where(testclasses==1)\n",
    "    pl.plot(testdata[0,which],testdata[1,which],'ko',ms=15)\n",
    "    pl.plot(testdata[0,which2],testdata[1,which2],'k^',ms=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_boost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging consists of the following steps:\n",
    "\n",
    "* create $T$ bootstrap samples $S_{i}$,\n",
    "\n",
    "* for each sample $S_{i}$ train a classifier,\n",
    "\n",
    "* vote $f(x)=\\text{arg}\\max\\sum_{i}^{T}(f_{i}(X)=y)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dtree:\n",
    "    \"\"\" A basic Decision Tree\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "\n",
    "    def read_data(self, filename):\n",
    "        fid = open(filename, \"r\")\n",
    "        data = []\n",
    "        d = []\n",
    "        for line in fid.readlines():\n",
    "            d.append(line.strip())\n",
    "        for d1 in d:\n",
    "            data.append(d1.split(\",\"))\n",
    "        fid.close()\n",
    "\n",
    "        self.featureNames = data[0]\n",
    "        self.featureNames = self.featureNames[:-1]\n",
    "        data = data[1:]\n",
    "        self.classes = []\n",
    "        for d in range(len(data)):\n",
    "            self.classes.append(data[d][-1])\n",
    "            data[d] = data[d][:-1]\n",
    "\n",
    "        return data, self.classes, self.featureNames\n",
    "\n",
    "    def classify(self, tree, datapoint):\n",
    "\n",
    "        if type(tree) == type(\"string\"):\n",
    "            # Have reached a leaf\n",
    "            return tree\n",
    "        else:\n",
    "            a = list(tree.keys())[0]\n",
    "            for i in range(len(self.featureNames)):\n",
    "                if self.featureNames[i] == a:\n",
    "                    break\n",
    "\n",
    "            try:\n",
    "                t = tree[a][datapoint[i]]\n",
    "                return self.classify(t, datapoint)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "    def classifyAll(self, tree, data):\n",
    "        results = []\n",
    "        for i in range(len(data)):\n",
    "            results.append(self.classify(tree, data[i]))\n",
    "        return results\n",
    "\n",
    "    def make_tree(self, data, classes, featureNames, maxlevel=-1, level=0, forest=0):\n",
    "        \"\"\" The main function, which recursively constructs the tree\"\"\"\n",
    "\n",
    "        nData = len(data)\n",
    "        nFeatures = len(data[0])\n",
    "\n",
    "        try:\n",
    "            self.featureNames\n",
    "        except:\n",
    "            self.featureNames = featureNames\n",
    "\n",
    "        # List the possible classes\n",
    "        newClasses = []\n",
    "        for aclass in classes:\n",
    "            if newClasses.count(aclass) == 0:\n",
    "                newClasses.append(aclass)\n",
    "\n",
    "        # Compute the default class (and total entropy)\n",
    "        frequency = np.zeros(len(newClasses))\n",
    "\n",
    "        totalEntropy = 0\n",
    "        totalGini = 0\n",
    "        index = 0\n",
    "        for aclass in newClasses:\n",
    "            frequency[index] = classes.count(aclass)\n",
    "            totalEntropy += self.calc_entropy(float(frequency[index]) / nData)\n",
    "            totalGini += (float(frequency[index]) / nData) ** 2\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        totalGini = 1 - totalGini\n",
    "        default = classes[np.argmax(frequency)]\n",
    "\n",
    "        if nData == 0 or nFeatures == 0 or (maxlevel >= 0 and level > maxlevel):\n",
    "            # Have reached an empty branch\n",
    "            return default\n",
    "        elif classes.count(classes[0]) == nData:\n",
    "            # Only 1 class remains\n",
    "            return classes[0]\n",
    "        else:\n",
    "\n",
    "            # Choose which feature is best\n",
    "            gain = np.zeros(nFeatures)\n",
    "            ggain = np.zeros(nFeatures)\n",
    "            featureSet = range(nFeatures)\n",
    "            if forest != 0:\n",
    "                np.random.shuffle(featureSet)\n",
    "                featureSet = featureSet[0:forest]\n",
    "            for feature in featureSet:\n",
    "                g, gg = self.calc_info_gain(data, classes, feature)\n",
    "                gain[feature] = totalEntropy - g\n",
    "                ggain[feature] = totalGini - gg\n",
    "\n",
    "            bestFeature = np.argmax(gain)\n",
    "            tree = {featureNames[bestFeature]: {}}\n",
    "\n",
    "            # List the values that bestFeature can take\n",
    "            values = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] not in values:\n",
    "                    values.append(datapoint[bestFeature])\n",
    "\n",
    "            for value in values:\n",
    "                # Find the datapoints with each feature value\n",
    "                newData = []\n",
    "                newClasses = []\n",
    "                index = 0\n",
    "                for datapoint in data:\n",
    "                    if datapoint[bestFeature] == value:\n",
    "                        if bestFeature == 0:\n",
    "                            newdatapoint = datapoint[1:]\n",
    "                            newNames = featureNames[1:]\n",
    "                        elif bestFeature == nFeatures:\n",
    "                            newdatapoint = datapoint[:-1]\n",
    "                            newNames = featureNames[:-1]\n",
    "                        else:\n",
    "                            newdatapoint = datapoint[:bestFeature]\n",
    "                            newdatapoint.extend(datapoint[bestFeature + 1:])\n",
    "                            newNames = featureNames[:bestFeature]\n",
    "                            newNames.extend(featureNames[bestFeature + 1:])\n",
    "                        newData.append(newdatapoint)\n",
    "                        newClasses.append(classes[index])\n",
    "                    index += 1\n",
    "\n",
    "                # Now recurse to the next level\n",
    "                subtree = self.make_tree(newData, newClasses, newNames, maxlevel, level + 1, forest)\n",
    "\n",
    "                # And on returning, add the subtree on to the tree\n",
    "                tree[featureNames[bestFeature]][value] = subtree\n",
    "\n",
    "            return tree\n",
    "\n",
    "    def printTree(self, tree, name):\n",
    "        if type(tree) == dict:\n",
    "            print(name, list(tree.keys())[0])\n",
    "            for item in list(list(tree.values())[0].keys()):\n",
    "                print(name, item)\n",
    "                self.printTree(list(tree.values())[0][item], name + \"\\t\")\n",
    "        else:\n",
    "            print\n",
    "            name, \"\\t->\\t\", tree\n",
    "\n",
    "    def calc_entropy(self, p):\n",
    "        if p != 0:\n",
    "            return -p * np.log2(p)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def calc_info_gain(self, data, classes, feature):\n",
    "\n",
    "        # Calculates the information gain based on both entropy and the Gini impurity\n",
    "        gain = 0\n",
    "        ggain = 0\n",
    "        nData = len(data)\n",
    "\n",
    "        # List the values that feature can take\n",
    "\n",
    "        values = []\n",
    "        for datapoint in data:\n",
    "            if datapoint[feature] not in values:\n",
    "                values.append(datapoint[feature])\n",
    "\n",
    "        featureCounts = np.zeros(len(values))\n",
    "        entropy = np.zeros(len(values))\n",
    "        gini = np.zeros(len(values))\n",
    "        valueIndex = 0\n",
    "        # Find where those values appear in data[feature] and the corresponding class\n",
    "        for value in values:\n",
    "            dataIndex = 0\n",
    "            newClasses = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] == value:\n",
    "                    featureCounts[valueIndex] += 1\n",
    "                    newClasses.append(classes[dataIndex])\n",
    "                dataIndex += 1\n",
    "\n",
    "            # Get the values in newClasses\n",
    "            classValues = []\n",
    "            for aclass in newClasses:\n",
    "                if classValues.count(aclass) == 0:\n",
    "                    classValues.append(aclass)\n",
    "\n",
    "            classCounts = np.zeros(len(classValues))\n",
    "            classIndex = 0\n",
    "            for classValue in classValues:\n",
    "                for aclass in newClasses:\n",
    "                    if aclass == classValue:\n",
    "                        classCounts[classIndex] += 1\n",
    "                classIndex += 1\n",
    "\n",
    "            for classIndex in range(len(classValues)):\n",
    "                entropy[valueIndex] += self.calc_entropy(float(classCounts[classIndex]) / np.sum(classCounts))\n",
    "                gini[valueIndex] += (float(classCounts[classIndex]) / np.sum(classCounts)) ** 2\n",
    "\n",
    "            # Computes both the Gini gain and the entropy\n",
    "            gain = gain + float(featureCounts[valueIndex]) / nData * entropy[valueIndex]\n",
    "            ggain = ggain + float(featureCounts[valueIndex]) / nData * gini[valueIndex]\n",
    "            valueIndex += 1\n",
    "        return gain, 1 - ggain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bagger:\n",
    "    \"\"\"The bagging algorithm based on the decision tree of Chapter 6\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.tree = dtree()\n",
    "\n",
    "    def bag(self, data, targets, features, nSamples):\n",
    "\n",
    "        nPoints = np.shape(data)[0]\n",
    "        nDim = np.shape(data)[1]\n",
    "        self.nSamples = nSamples\n",
    "\n",
    "        # Compute bootstrap samples\n",
    "        samplePoints = np.random.randint(0, nPoints, (nPoints, nSamples))\n",
    "        classifiers = []\n",
    "\n",
    "        for i in range(nSamples):\n",
    "            sample = []\n",
    "            sampleTarget = []\n",
    "            for j in range(nPoints):\n",
    "                sample.append(data[samplePoints[j, i]])\n",
    "                sampleTarget.append(targets[samplePoints[j, i]])\n",
    "            # Train classifiers\n",
    "            classifiers.append(self.tree.make_tree(sample, sampleTarget, features, 1))\n",
    "\n",
    "        return classifiers\n",
    "\n",
    "    def bagclass(self, classifiers, data):\n",
    "\n",
    "        decision = []\n",
    "        # Majority voting\n",
    "        for j in range(len(data)):\n",
    "            outputs = []\n",
    "            # print data[j]\n",
    "            for i in range(self.nSamples):\n",
    "                out = self.tree.classify(classifiers[i], data[j])\n",
    "                if out is not None:\n",
    "                    outputs.append(out)\n",
    "            # List the possible outputs\n",
    "            out = []\n",
    "            for each in outputs:\n",
    "                if out.count(each) == 0:\n",
    "                    out.append(each)\n",
    "            frequency = np.zeros(len(out))\n",
    "\n",
    "            index = 0\n",
    "            if len(out) > 0:\n",
    "                for each in out:\n",
    "                    frequency[index] = outputs.count(each)\n",
    "                    index += 1\n",
    "                decision.append(out[frequency.argmax()])\n",
    "            else:\n",
    "                decision.append(None)\n",
    "        return decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  safety\n",
      "  low\n",
      "  high\n",
      " \t persons\n",
      " \t 2\n",
      " \t 4\n",
      " \t\t buying\n",
      " \t\t vhigh\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t high\n",
      " \t\t\t med\n",
      " \t\t\t low\n",
      " \t\t high\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t high\n",
      " \t\t\t med\n",
      " \t\t\t low\n",
      " \t\t med\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t high\n",
      " \t\t\t med\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t low\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t low\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t high\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t med\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t low\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t more\n",
      " \t\t buying\n",
      " \t\t vhigh\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t high\n",
      " \t\t\t med\n",
      " \t\t\t\t doors\n",
      " \t\t\t\t 2\n",
      " \t\t\t\t\t luggage\n",
      " \t\t\t\t\t small\n",
      " \t\t\t\t\t big\n",
      " \t\t\t\t 3\n",
      " \t\t\t\t 4\n",
      " \t\t\t\t 5more\n",
      " \t\t\t low\n",
      " \t\t\t\t doors\n",
      " \t\t\t\t 2\n",
      " \t\t\t\t\t luggage\n",
      " \t\t\t\t\t small\n",
      " \t\t\t\t\t big\n",
      " \t\t\t\t 3\n",
      " \t\t\t\t 4\n",
      " \t\t\t\t 5more\n",
      " \t\t high\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t high\n",
      " \t\t\t\t doors\n",
      " \t\t\t\t 2\n",
      " \t\t\t\t\t luggage\n",
      " \t\t\t\t\t small\n",
      " \t\t\t\t\t big\n",
      " \t\t\t\t 3\n",
      " \t\t\t\t 4\n",
      " \t\t\t\t 5more\n",
      " \t\t\t med\n",
      " \t\t\t\t doors\n",
      " \t\t\t\t 2\n",
      " \t\t\t\t\t luggage\n",
      " \t\t\t\t\t small\n",
      " \t\t\t\t\t big\n",
      " \t\t\t\t 3\n",
      " \t\t\t\t 4\n",
      " \t\t\t\t 5more\n",
      " \t\t\t low\n",
      " \t\t\t\t doors\n",
      " \t\t\t\t 2\n",
      " \t\t\t\t\t luggage\n",
      " \t\t\t\t\t small\n",
      " \t\t\t\t\t big\n",
      " \t\t\t\t 3\n",
      " \t\t\t\t 4\n",
      " \t\t\t\t 5more\n",
      " \t\t med\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t\t doors\n",
      " \t\t\t\t 2\n",
      " \t\t\t\t\t luggage\n",
      " \t\t\t\t\t small\n",
      " \t\t\t\t\t big\n",
      " \t\t\t\t 3\n",
      " \t\t\t\t 4\n",
      " \t\t\t\t 5more\n",
      " \t\t\t high\n",
      " \t\t\t\t doors\n",
      " \t\t\t\t 2\n",
      " \t\t\t\t\t luggage\n",
      " \t\t\t\t\t small\n",
      " \t\t\t\t\t big\n",
      " \t\t\t\t 3\n",
      " \t\t\t\t 4\n",
      " \t\t\t\t 5more\n",
      " \t\t\t med\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t small\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t big\n",
      " \t\t\t\t med\n",
      " \t\t\t low\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t small\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t big\n",
      " \t\t\t\t med\n",
      " \t\t low\n",
      " \t\t\t luggage\n",
      " \t\t\t small\n",
      " \t\t\t\t doors\n",
      " \t\t\t\t 2\n",
      " \t\t\t\t 4\n",
      " \t\t\t\t\t maintenance\n",
      " \t\t\t\t\t vhigh\n",
      " \t\t\t\t\t high\n",
      " \t\t\t\t\t med\n",
      " \t\t\t\t\t low\n",
      " \t\t\t big\n",
      " \t\t\t\t maintenance\n",
      " \t\t\t\t vhigh\n",
      " \t\t\t\t high\n",
      " \t\t\t\t med\n",
      " \t\t\t\t low\n",
      " \t\t\t med\n",
      " \t\t\t\t maintenance\n",
      " \t\t\t\t vhigh\n",
      " \t\t\t\t high\n",
      " \t\t\t\t med\n",
      " \t\t\t\t low\n",
      "  med\n",
      " \t persons\n",
      " \t 2\n",
      " \t 4\n",
      " \t\t buying\n",
      " \t\t vhigh\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t high\n",
      " \t\t\t med\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 3\n",
      " \t\t\t\t\t 5more\n",
      " \t\t\t low\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 3\n",
      " \t\t\t\t\t 5more\n",
      " \t\t high\n",
      " \t\t\t luggage\n",
      " \t\t\t small\n",
      " \t\t\t big\n",
      " \t\t\t\t maintenance\n",
      " \t\t\t\t vhigh\n",
      " \t\t\t\t high\n",
      " \t\t\t\t med\n",
      " \t\t\t\t low\n",
      " \t\t\t med\n",
      " \t\t\t\t doors\n",
      " \t\t\t\t 3\n",
      " \t\t\t\t 5more\n",
      " \t\t\t\t\t maintenance\n",
      " \t\t\t\t\t vhigh\n",
      " \t\t\t\t\t high\n",
      " \t\t\t\t\t med\n",
      " \t\t\t\t\t low\n",
      " \t\t med\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 3\n",
      " \t\t\t\t\t 5more\n",
      " \t\t\t high\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 3\n",
      " \t\t\t\t\t 5more\n",
      " \t\t\t med\n",
      " \t\t\t low\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 3\n",
      " \t\t\t\t\t 5more\n",
      " \t\t low\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 3\n",
      " \t\t\t\t\t 5more\n",
      " \t\t\t high\n",
      " \t\t\t med\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 3\n",
      " \t\t\t\t\t 5more\n",
      " \t\t\t low\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 3\n",
      " \t\t\t\t\t 5more\n",
      " \t more\n",
      " \t\t buying\n",
      " \t\t vhigh\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t high\n",
      " \t\t\t med\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t low\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t high\n",
      " \t\t\t luggage\n",
      " \t\t\t med\n",
      " \t\t\t\t doors\n",
      " \t\t\t\t 2\n",
      " \t\t\t\t 4\n",
      " \t\t\t\t\t maintenance\n",
      " \t\t\t\t\t vhigh\n",
      " \t\t\t\t\t high\n",
      " \t\t\t\t\t med\n",
      " \t\t\t\t\t low\n",
      " \t\t\t small\n",
      " \t\t\t big\n",
      " \t\t\t\t maintenance\n",
      " \t\t\t\t vhigh\n",
      " \t\t\t\t high\n",
      " \t\t\t\t med\n",
      " \t\t\t\t low\n",
      " \t\t med\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t high\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t med\n",
      " \t\t\t low\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t low\n",
      " \t\t\t maintenance\n",
      " \t\t\t vhigh\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t high\n",
      " \t\t\t med\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      " \t\t\t low\n",
      " \t\t\t\t luggage\n",
      " \t\t\t\t med\n",
      " \t\t\t\t\t doors\n",
      " \t\t\t\t\t 2\n",
      " \t\t\t\t\t 4\n",
      " \t\t\t\t small\n",
      " \t\t\t\t big\n",
      "Tree\n",
      "Number correctly predicted 777.0\n",
      "Number of testpoints  864\n",
      "Percentage Accuracy  89.93055555555556\n",
      "\n",
      "Number of cars rated as good or very good 39.0\n",
      "Number correctly identified as good or very good 18.0\n",
      "Percentage Accuracy 46.15384615384615\n",
      "-----\n",
      "Bagger\n",
      "Number correctly predicted 678.0\n",
      "Number of testpoints  864\n",
      "Percentage Accuracy  78.47222222222221\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tree = dtree()\n",
    "bag  = bagger()\n",
    "\n",
    "data,classes,features = tree.read_data('car.data')\n",
    "\n",
    "train = data[::2][:]\n",
    "test = data[1::2][:]\n",
    "trainc = classes[::2]\n",
    "testc = classes[1::2]\n",
    "\n",
    "t=tree.make_tree(train,trainc,features)\n",
    "out = tree.classifyAll(t,test)\n",
    "tree.printTree(t,' ')\n",
    " \n",
    "a = np.zeros(len(out))\n",
    "b = np.zeros(len(out))\n",
    "d = np.zeros(len(out))\n",
    " \n",
    "for i in range(len(out)):\n",
    "    if testc[i] == 'good' or testc[i]== 'v-good':\n",
    "        b[i] = 1\n",
    "        if out[i] == testc[i]:\n",
    "            d[i] = 1\n",
    "    if out[i] == testc[i]:\n",
    "        a[i] = 1\n",
    "print(\"Tree\")\n",
    "print(\"Number correctly predicted\",str(np.sum(a)))\n",
    "print(\"Number of testpoints \",str(len(a)))\n",
    "print(\"Percentage Accuracy \",str(np.sum(a)/len(a)*100.0))\n",
    "print(\"\")\n",
    "print(\"Number of cars rated as good or very good\", str(np.sum(b)))\n",
    "print(\"Number correctly identified as good or very good\",str(np.sum(d)))\n",
    "print(\"Percentage Accuracy\",str(np.sum(d)/np.sum(b)*100.0))\n",
    " \n",
    "c=bag.bag(train,trainc,features,100)\n",
    "out = bag.bagclass(c,test)\n",
    " \n",
    "a = np.zeros(len(out))\n",
    "b = np.zeros(len(out))\n",
    "d = np.zeros(len(out))\n",
    " \n",
    "for i in range(len(out)):\n",
    "    if testc[i] == 'good' or testc[i]== 'v-good':\n",
    "        b[i] = 1\n",
    "        if out[i] == testc[i]:\n",
    "            d[i] = 1\n",
    "    if out[i] == testc[i]:\n",
    "        a[i] = 1\n",
    "print(\"-----\")\n",
    "print(\"Bagger\")\n",
    "print(\"Number correctly predicted\",str(np.sum(a)))\n",
    "print(\"Number of testpoints \",str(len(a)))\n",
    "print(\"Percentage Accuracy \",str(np.sum(a)/len(a)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework. Random Forest. \n",
    "\n",
    "Build a random forest based on dtree class. Use the same dataset cars for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    def __init__(self, n_trees, n_features, max_depth, dtree):\n",
    "        self.n_trees = n_trees\n",
    "        self.dtree = dtree\n",
    "        self.n_features = n_features\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.trees = list()\n",
    "        for i in range(self.n_trees):\n",
    "            tree = self.dtree.make_tree(X_train, y_train, features, self.max_depth, self.n_features)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def score(self, X_test, y_test):\n",
    "        predictions = self._predict(X_test)\n",
    "        \n",
    "        correct = 0\n",
    "        for i in range(len(y_test)):\n",
    "            if y_test[i] == predictions[i]:\n",
    "                correct += 1\n",
    "        \n",
    "        return correct * 100 / len(y_test)\n",
    "    \n",
    "    def _predict(self, X_test):\n",
    "        predictions = list()\n",
    "           \n",
    "        for row in X_test:\n",
    "            for tree in self.trees:\n",
    "                res = self.dtree.classify(tree, row)\n",
    "                predictions.append(res)\n",
    "                \n",
    "        return predictions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-81480258584b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mrandom_forest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3d6c1508134a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-772cd216529d>\u001b[0m in \u001b[0;36mmake_tree\u001b[0;34m(self, data, classes, featureNames, maxlevel, level, forest)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mbestFeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mfeatureNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbestFeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# List the values that bestFeature can take\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "random.seed(2)\n",
    "\n",
    "tree = dtree()\n",
    "bag  = bagger()\n",
    "\n",
    "data, classes, features = tree.read_data('car.data')\n",
    "\n",
    "train_data = data[::2][:]\n",
    "test_data = data[1::2][:]\n",
    "train_classes = classes[::2]\n",
    "test_classes = classes[1::2]\n",
    "\n",
    "features = int(sqrt(len(train_data[0])))\n",
    "random_forest = RandomForest(10, features, 15, tree)\n",
    "\n",
    "random_forest.fit(train_data, train_classes)\n",
    "random_forest.score(test_data, test_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
